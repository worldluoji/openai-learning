# GPT
GPT 的英文全称翻译过来就是“生成式预训练 Transformer（Generative Pre-trained Transformer）

预训练模型，就是虽然我们没有看过你想要解决的问题，比如在情感分析里看到的用户评论和评分。但是，我可以拿很多我能找到的文本，比如网页文章、维基百科里的文章，各种书籍的电子版等等，作为理解文本内容的一个学习资料。

可以这样来理解：用来训练的语料文本越丰富，模型中可以放的参数越多，那模型能够学到的关系也就越多。类似的情况在文本里出现得越多，那么将来模型猜得也就越准。

早在 2013 年，就有一篇叫做 Word2Vec 的经典论文谈到过。它能够通过预训练，根据同一个句子里一个单词前后出现的单词，来得到每个单词的向量。而在 2018 年，Google 关于 BERT 的论文发表之后，整个业界也都会使用 BERT 这样的预训练模型，把一段文本变成向量用来解决自己的自然语言处理任务。在 GPT-3 论文发表之前，大家普遍的结论是，BERT 作为预训练的模型效果也是优于 GPT 的。

除了OpenAI外，Meta和Google也有自己GPT:
- Meta的 Fasttext，它继承了 Word2Vec 的思路，能够把一个个单词表示成向量。
- Google 的 T5，T5 的全称是 Text-to-Text Transfer Trasnformer，是适合做迁移学习的一个模型。所谓迁移学习，也就是它推理出来向量的结果，常常被拿来再进行机器学习，去解决其他自然语言处理问题。通常很多新发表的论文，会把 T5 作为预训练模型进行微调和训练.

<br>

## 大语言模型的两种模式
大语言模型的原理，就是利用训练样本里面出现的文本的前后关系，通过前面的文本对接下来出现的文本进行概率预测。如果类似的前后文本出现得越多，那么这个概率在训练过程里会收敛到少数正确答案上，回答就准确。如果这样的文本很少，那么训练过程里就会有一定的随机性，对应的答案就容易似是而非。

在 GPT-3 的模型里，虽然整体的训练语料很多，但是中文语料很少。只有不到 1% 的语料是中文的，所以如果问很多中文相关的知识性或者常识性问题，它的回答往往就不准确。

1. 一个解决办法，就是<strong>多找一些高质量的中文语料训练一个新的模型</strong>。或者，对于我们想让 AI 能够回答出来的问题，找一些数据。然后利用 OpenAI 提供的“微调”（Fine-tune）接口，在原来的基础上训练一个新模型出来。
如果是时效性要求比较强的资讯类的信息，就很难这么做。比如，我们想让 AI 告诉我们前一天足球赛的比分，我们不太可能每隔几个小时就单独训练或者微调一下模型，那样干的成本太高了。

2. Bing 的解法——先搜索，后提示.
先通过搜索的方式，找到和询问的问题最相关的语料。这个搜索过程中，我们既可以用传统的基于关键词搜索的技术，也可以用 Embedding 的相似度进行语义搜索的技术。然后，将和问题语义最接近的前几条内容，作为提示语的一部分给到 AI。然后请 AI 参考这些内容，再来回答这个问题。（我们不必从 0 开始写代码。因为这个模式实在太过常用了，所以有人为它写了一个开源 Python 包，叫做 llama-index,
demo: 8. llama-index）

这也揭示了大语言模型其实内含了两种能力：
- 第一种，是海量的语料中，本身已经包含了的知识信息。比如，我们前面问 AI 鱼香肉丝的做法，它能回答上来就是因为语料里已经有了充足的相关知识。我们一般称之为“世界知识”。
- 第二种，是根据你输入的内容，理解和推理的能力。这个能力，不需要训练语料里有一样的内容。而是大语言模型本身有“思维能力”，能够进行阅读理解。这个过程里，“知识”不是模型本身提供的，而是我们找出来，临时提供给模型的。如果不提供这个上下文，再问一次模型相同的问题，它还是答不上来的。

总结一下，通过llama-index ，将外部的资料库索引起来进行问答; 通过 Langchain（9.LLMChain）的链式调用，实时获取外部的数据信息，或者运行 Python 程序, 以及记住对话中我们关心的部分。将这些能力组合起来，我们就可以搭建一个完整的，属于自己的聊天机器人。


## GPT 发展历史概述
GPT的发展历史可以追溯到2013年，当时谷歌首先提出了使用深度学习技术来训练大规模的神经网络，以实现自然语言处理的目标。在接下来的几年中，这种深度学习技术得到了进一步的发展和应用，包括使用循环神经网络（RNN）和长短时记忆网络（LSTM）等结构来处理序列数据。

到了2018年，GPT-1诞生了，它是一种基于Transformer结构的语言模型，使用了自注意力机制来实现对上下文信息的有效捕捉。
GPT-1在当时是一个非常庞大的模型，拥有12层结构，共包含了8600万个参数。
然而，GPT-1还只能用于监督和任务无关的任务，其性能表现还比较有限。

到了2019年，OpenAI又推出了GPT-2。GPT-2在模型结构上进行了改进，增加了更多的参数，达到了1.5亿个参数，
同时采用了更复杂的训练方法和技巧。GPT-2在自然语言推理、问答、文本分类等任务上的表现都有了明显的提升。
此外，GPT-2还被发现具有很强的生成能力，能够生成高质量的文本，这也为后续的应用提供了可能。

到了2020年，GPT-3问世了。GPT-3在模型结构上又进行了升级，达到了3亿个参数，同时在训练方法和技巧上也更加复杂。
GPT-3在各种自然语言处理任务上的表现都达到了很高的水平，同时它还具备了很强的生成能力，可以生成各种类型的文本，
包括摘要、故事、诗歌等。此外，GPT-3还引入了RLHF技术，使得模型能够更好地理解人类的意图和情感。

到了2022年，ChatGPT被开发出来。ChatGPT是基于GPT-3.5架构的大型语言模型，通过强化学习进行了训练。
它能够进行更加自然、流畅的对话，同时还可以理解并回应人类的情感和需求。
ChatGPT的推出在人工智能领域引起了轰动，被视为大型语言模型发展的一个重要里程碑。


GPT-4于2023年3月14日发布，是GPT-3.5的后继者。GPT-4使用了更强大的1.8万亿个参数，并从更大的1PB数据集中学习。
它还采用了更先进的Transformer架构，包括稀疏注意力、可逆层和激活检查点等技术，以减少内存消耗和计算成本。

与GPT-3相比，GPT-4在各种主题和任务上生成自然语言文本的能力得到了显著提升。
它能够更好地理解人类意图和情感，同时还可以生成更自然、更流畅的对话。
GPT-4还具备更强的学习能力，可以通过与人类的互动和反馈进行学习和改进。


## GPT VS Bert

<img src="./theory/images/GPT%20VS%20Bert.png" />

<img src="./theory/images/GPT%20and%20Bert.png" />