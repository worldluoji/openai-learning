# 多模态的 CLIP 模型
所谓“多模态”，就是多种媒体形式的内容。 CLIP 这个模型，就是一个多模态模型。
一如即往，OpenAI 仍然是通过海量数据来训练一个大模型。整个模型使用了互联网上的 4 亿张图片，它不仅能够分别理解图片和文本，还通过对比学习建立了图片和文本之间的关。

CLIP 的思路其实不复杂，就是利用互联网上已有的大量公开的图片数据。而且其中有很多已经通过 HTML 标签里面的 title 或者 alt 字段，提供了对图片的文本描述。
```
<img src="img_girl.jpg" alt="Girl in a jacket" width="500" height="600">

<img src="/img/html/vangogh.jpg"
     title="Van Gogh, Self-portrait.">
```
我们只要训练一个模型，将文本转换成一个向量，也将图片转换成一个向量。图片向量应该和自己的文本描述向量的距离尽量近，和其他的文本向量要尽量远。那么这个模型，就能够把图片和文本映射到同一个空间里。我们就能够通过向量同时理解图片和文本了。

demo1: dog-or-cat.py