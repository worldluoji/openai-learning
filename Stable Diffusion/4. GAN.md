# GAN
在 22 年以前，GAN 才是业界公认的 AI 绘画技术首选，它也可以进行图像生成、局部编辑、图像风格化、老照片修复等。
在老一辈的 AI 画图中，GAN（生成对抗网络）可以说是唯一的选择。
相信你也在各种社交软件上见到过各种变小孩、变老、性别变换的视觉特效，这类效果通常就是靠 GAN 完成的。

随着 22 年 DALL-E 2、Stable Diffusion 的推出，扩散模型技术逐渐成为了 AI 绘画的主流技术。
无论是绘画细节的精致度还是内容的多样性，扩散模型似乎都要优于 GAN。

搞懂了 GAN 的长处和短板，才能理解后来扩散模型解决了 GAN 的哪些痛点。

<br>

## GAN 的起源
2014 年， Ian Goodfellow 等人提出了生成对抗网络——也就是 GAN 这个全新的概念。

GAN 模型由两个模块构成，也就是常说的生成器（Generator）和判别器（Discriminator）。
可以这样类比，生成器是一位名画伪造家，目标是创作出逼真的艺术品，判别器是一位艺术鉴赏家，目标是从细节中找出伪造破绽。

生成器与判别器在模型训练的过程中持续更新与对抗，最终达到平衡。

GAN 模型存在一些问题，比如同时训练生成器和判别器的过程并不稳定，最初的生成器生成内容不能被指定，
生成的图像分辨率较低，模型推理在手机等设备上用时过长等等。

<br>

## 图像生成能力的进化：DCGAN/CGAN/WGAN
2015 年由 Radford 等人提出的深度卷积 GAN（DCGAN）给 GAN 带来了进化可能。
主要创新就是<strong>引入卷积神经网络（CNN）结构</strong>，通过卷积层和反卷积层替代全连接层，使得生成器和判别器能够感知和利用图像的局部关系，
更好地处理图像数据，从而生成更逼真的图像。

DCGAN 的优点在于它的稳定性和生成效果。通过使用卷积神经网络，DCGAN 能够更好地保持图像的空间结构和细节信息，生成的图像质量更高。

条件 GAN，简称 cGAN，允许我们在生成图像的过程中<strong>引入额外的条件信息</strong>。这样一来，我们可以控制生成图像的特征，比如生成特定类别的图像。
比如在上面的数字图中，普通的 GAN 无法提前指定生成的数字是 0 到 9 中的哪一个，而 cGAN 便可以轻松控制要生成的数字是几。

Wasserstein GAN，简称 wGAN，是另一个重要的改进，它通过使用 Wasserstein 距离（瓦瑟斯坦距离，也被称为地面距离）
来衡量生成图像和真实图像之间的差异，这样就能提升训练的稳定性和生成图像的质量。

cGAN 和 wGAN 生成图像的分辨率很低，分辨率提升是图像生成领域一个持续研究的方向，后来的 PGGAN、BigGAN、StyleGAN 等工作，
将生成图像的分辨率提高了 1024x1024 分辨率之上。

<br>

## 手机端实时特效：从 Pix2Pix 到 CycleGAN
Pix2Pix 系列工作延续了 cGAN 的思想，将 cGAN 的条件换成了与原图尺寸大小相同的图片，可以实现类似轮廓图转真实图片、黑白图转彩色图等效果。
是不是听起来很熟悉？没错，就是 GAN 时代的 ControlNet！

Pix2Pix 最大的缺点就是训练需要大量目标图像与输入图像的图像对，优点是模型可以做到很轻很快，甚至能在很低端的手机上也能达到实时效果。
从 18 年至今，我们在短视频平台上看到的各种实时变脸特效，比如年龄转换、性别编辑等特效，都是基于这个技术。

但获取成对的数据是困难且耗时的，那大量成对数据该怎么来呢？答案就是大名鼎鼎的 CycleGAN。
2017 年 Jun-Yan Zhu 等人提出了 CycleGAN，也就是循环一致性生成对抗网络。

CycleGAN 的核心要点就是让两个不同领域的图像可以互相转换。它有两个生成器，分别是 G（A→B）和 G（B→A），它们的任务是把 A 领域的图像变成 B 领域的，反之亦然。
同时，还有两个判别器，D_A 和 D_B，负责分辨 A 和 B 领域里的真实图像和生成的图像。

CycleGAN 的关键点在于循环一致性损失。这个方法把原图像转换到目标领域，然后再转换回原来的领域，就可以确保生成的图像跟原图像差别不大。
这种循环一致性约束让图像转换有了双向的一致性。我举个例子你就明白了，先把马变成斑马，再恢复成马，最后的图像应该跟原来的马图像很相似。

<br>

## 高分辨率的生成：StyleGAN 系列工作
英伟达在 2018 年提出的生成对抗网络模型 StyleGAN，彻底改变了 GAN 在图像合成和风格迁移方面的应用前景。
与传统的 GAN 模型相比，StyleGAN 在图像生成的质量、多样性和可控性方面取得了显著的突破。

StyleGAN 的核心思想是: 用风格向量来控制生成图像的各种属性特点，并通过自适应实例归一化（AdaIN）把风格向量和生成器的特征图结合在一起。
另外，用渐进式的生成器结构逐渐提高分辨率，这样可以提高训练的稳定性和生成图像的质量。

StyleGAN 2 和 StyleGAN 3 是 StyleGAN 的改进版本。
它在 StyleGAN 的基础上引入了一系列重要的改进，进一步提升了图像生成的质量、稳定性和控制性。

另外还有一种叫做超分辨率生成对抗网络（SRGAN）的模型，它的目标是将低分辨率图像转换成高分辨率的图像。

<br>

## GigaGAN
GigaGAN 是一种具有突破性的 GAN 模型，它通过扩大模型规模，在多个方面展现了卓越的优势。
比如，对于 512 分辨率图像的合成，仅需要 0.13 秒的推理速度，这比现有的工作在推理速度上高出了一个数量级。
并且 GigaGAN 可以合成更高分辨率的图像，生成 1600 万像素的图像仅需 3.66 秒。