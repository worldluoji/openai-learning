# DALL-E
2022 年 4 月由 OpenAI 发布 AI 绘画模型DALL-E 2，它的绘画效果相比过去的工作有了质的飞跃，而且它提出的 **unCLIP 结构、图像变体能力**也被后来的方法所效仿。DALL-E这个名字灵感来源于超现实主义艺术家萨尔瓦多·达利（Salvador Dalí）和皮克斯动画电影《机器人总动员》中的角色瓦力（WALL-E），巧妙结合了艺术与科技的概念。

如果你想体验 OpenAI 的 DALL-E 2 原汁原味的效果，可以使用 Edge 浏览器 NewBing 的 AI 绘画能力。

除了基于文本描述生成高清图像的基本能力，DALL-E 2 还有其他好玩的功能。
- 第一种玩法就是生成图像变体，我们可以对经典画作进行“魔改”。像后面这样，输入一张图片，DALL-E 2 可以保留图像中的关键信息，并生成图像的变体。
- 第二种玩法就是图像的融合。比如输入两张图片，DALL-E 2 可以对两张图片进行插值，生成融合后的新图片。这里的融合既可以是图像风格上的融合，也可以是图像内容上的融合，或者是图像内容和图像风格二者的融合。
- 第三种是局部编辑。输入一张图片，DALL-E 2 可以根据我们的指令局部编辑图像，让我们仿佛拥有了指令级 PS 的能力。

<img src="./images/DALL-E PS.png" />

### 功能特点
- **文本到图像生成**：用户只需输入文本描述，如“穿着宇航服的猫站在月球上”，DALL-E就能生成相应的图像。
- **创意表达**：支持生成多样化的图像风格，包括但不限于现实主义、抽象艺术、未来主义等，极大地拓展了创意设计的可能性。
- **高分辨率与逼真度**：DALL-E 2版本进一步提高了图像的生成质量，能够输出更高分辨率且更为逼真的图像。
- **多模态理解**：展示了AI在理解语言和视觉信息结合方面的强大能力，是多模态AI研究的前沿成果。

### DALL-E 2原理
DALL-E 2 就是个“缝合怪”。我们把 CLIP 和扩散模型，或者 CLIP 和自回归模型组合到一起，引入大量图像文本数据进行模型训练后，便得到了 DALL-E 2。

<img src="./images/DALL-E2 principal.webp" />

先看标记 1。它是原始的 CLIP 结构，这部分在 DALL-E 2 的链路中是固定住的，无需训练。

然后是标记 2。它代表 CLIP 的文本编码器提取的文本表征。文本表征是指根据输入的文本，用 CLIP 文本编码器提取得到的数值特征。图中三处标记为 2 的表征是完全相同的。

标记 3 和标记 4 是两种可以互相替代的方案。标记 3 代表需要训练的自回归先验模型，也称为 Prior 模型。先验模型的作用，就是将 CLIP 提取的文本表征转换为 CLIP 图像表征。
  
注意，这里提到的 CLIP 文本表征是用 CLIP 文本编码器从文本中提取的，而这里提到的 CLIP 图像表征是经过先验模型预测得到的，而不是 CLIP 图像编码器提取的，我们可以将其视为类似于 CLIP 图像编码器提取的图像表征。

标记 4 表示需要训练的扩散先验模型。DALL-E 2 经过实验验证，使用扩散先验模型和自回归先验模型在生成效果上差不太多，扩散先验模型在计算效率上更有优势。既然原论文主要是围绕扩散先验展开的，所以我们也主要来学习扩散先验的用法。

最后来看标记 5 和标记 6，完成这两步以后文本表征才能转化成真正的图像。标记 5 是先验模块输出的图像表征，该表征类似于 CLIP 图像编码器提取的特征。标记 6 则是扩散模型，作用是将上一步得到的图像表征转换为图像。

DALL-E 2 的结构就非常简单明朗了。可以归纳为后面这三个步骤。
- 首先，使用一个预训练好的 CLIP 文本编码器将文本描述映射为文本表征。
- 然后，训练一个扩散先验模型，将文本表征映射为对应的图像表征。
- 最后，训练一个基于扩散模型的图像解码器，该解码器可以基于图像表征生成图像。

到这里，距离完全理解 DALL-E 2，我们还剩下两个最关键的问题。第一，扩散先验模型要如何训练呢？第二，扩散模型图像解码器要如何训练呢？

### 扩散先验模型该如何训练
扩散先验模型的目标是将 CLIP 文本表征转换为 CLIP 图像表征。DALL-E 2 的扩散先验模型并没有使用 UNet 结构，而是直接使用一个 Transformer 解码器。

UNet 结构擅长解决图像分割问题，因为 UNet 的输入和输出都是类似于图像的特征图；而 Transformer 的输入输出是序列化的特征，更适合完成 CLIP 文本表征到图像表征的转换。基于 Transformer 的扩散先验并不是预测每一步的噪声值，而是**直接预测每一步去噪后的图像表征**。

基于图像 - 文本成对训练数据，扩散先验模型的训练可以分为以下几个步骤。
- 首先，使用预训练好的 CLIP 文本编码器提取文本描述的文本表征。
- 然后，使用预训练好的 CLIP 图像编码器提取对应的图像表征。
- 之后，随机采样一个时间步 t，以时间步 t、CLIP 提取的文本表征、加噪之后的图像表征作为条件，基于 Transformer 去预测下一步去噪后的图像表征。

<img src="./images/DALL-E2 Diffusion Prior Model.png" />

图像解码器得到的输出是 64x64 分辨率的图像，这对于我们来说是远远不够的。因此，作者又设计了两个连续的上采样器，实现图像的超分辨率处理，最终得到 1024x1024 的高清图像。我们需要留意的是，这里用到的两个上采样器都是扩散模型。

<br>

### 图像变体
我们常常把 DALL-E 2 的文生图方案称为 unCLIP，这种方案和 Stable Diffusion 的技术链路是不同的。unCLIP 模型有一些独特的优势，比如用户输入一张图像，unCLIP 模型可以保留原始图像的关键信息，生成一系列图像变体。

前面已经说过，预训练好的 CLIP 图像编码器可以提取到图像表征，文本编码器可以提取到文本表征。DALL-E 2 从图像表征直接生成图像，正好和 CLIP 从图像提取图像表征是一个相反的过程，这个过程被称为 unCLIP。

<img src="./images/image represent.png" />

用户输入一张图像，使用 CLIP 的图像编码器提取图像表征作为图像解码器的输入，这样就实现了生成图像变体的能力。前面我们提到扩散先验模型的作用是得到类似于 CLIP 图像编码器提取的图像表征，而图像变体功能使用 CLIP 图像编码器提取图像表征，二者是类似的。unCLIP 的精妙之处就在这里，你可以仔细品味一下。

虽然 CLIP 的图像编码器是固定的，从给定原始图像得到的图像表征是一个确定性的过程。但扩散解码器每次从随机噪声出发，结合 CLIP 图像表征会生成变体图像，这个生成图像的过程是有随机性的，所以 DALL-E 2 才会轻易得到很多变体图像。

<br>

## 局限性

当然，任何事物都不是完美的，DALL-E 2 这个工作也存在一些局限性。

首先，DALL-E 2 并不擅长处理逻辑关系。比如我们要求其生成“蓝色方块上面放一个红色的方块”，对比 GLIDE 这个论文，DALL-E 2 得到的效果并不准确。

其次，DALL-E 2 并不擅长在生成的图像中写出要求的文字。比如我们要求 DALL-E 2 生成一幅图，“写着 Deep Learning 的标志板”，结果并不理想。

此外，DALL-E 2 不擅长生成复杂的场景，细节不足。比如我们要求其生成“一张高质量的时代广场的照片”，得到的结果很粗糙。

除了上面这些，DALL-E 2 生成的内容还存在一些种族偏见，比如一些特定的职业会生成特定的有色人种；DALL-E 2 也会生成一些色情暴力的内容，这是由于训练的数据中包括这类内容。

这些问题的存在孕育了更多优秀的改善思路，涌现出更多解决方案，随着我们学习的深入，你会发现在过去的一年里上述问题都得到了较大的改善。

### 应用前景
DALL-E的应用前景广阔，涵盖了创意产业、教育、广告、娱乐等多个领域。例如，它可以帮助设计师快速可视化概念设计，辅助艺术家创作，或是作为教学工具来增强学习体验。此外，它还促进了对AI生成艺术的讨论，以及对版权、原创性和人工智能伦理的思考。