# 训练一个自己的扩散模型
## 关键知识串联
标准扩散模型的训练过程包含 6 个步骤，分别是随机选取训练图像、随机选择时间步 t、随机生成高斯噪声、一步计算第 t 步加噪图、使用 UNet 预测噪声值和计算噪声数值误差。

Stable Diffusion 在此基础上，增加了 VAE 模块和 CLIP 模块。VAE 模块的作用是降低输入图像的维度，从而加快模型训练、给 GPU 腾腾地方；CLIP 模块的作用则是将文本描述通过交叉注意力机制注入到 UNet 模块，让 AI 绘画模型做到言出法随。

<img src="./images/stable diffusion UNet.webp" />

在 Stable Diffusion 中，还有很多其他黑魔法，比如无条件引导控制（Classifier-Free Guidance）、引导强度（Guidance Scale）等

## 训练一个标准扩散模型和 Stable Diffusion 模型，需要准备哪些“原材料”呢？
首先，我们需要 GPU，显存越大越好。

然后，我们需要训练数据。对于标准扩散模型而言，我们只需要纯粹的图片数据即可；对于 Stable Diffusion，由于我们需要文本引导，就需要用到图片数据对应的文本描述。这里的文本描述既可以是像 CLIP 训练数据那种对应的文本描述，也可以是使用各种图片描述（image caption）模型获取的文本描述。

从头开始训练的成本差不多是几套海淀学区房的价格，所以我们最好是基于某个开源预训练模型进行针对性微调。事实上，开源社区里大多数模型都是微调出来的。

## 训练扩散模型
这里我们通过两种方式来训练扩散模型。

- 第一种是使用 denoising_diffusion_pytorch 这个高度集成的工具包。
- 第二种则是基于 diffusers 这种更多开发者使用的工具包。

对于专业的算法同学而言，我更推荐使用 diffusers 来训练。原因是 diffusers 工具包在实际的 AI 绘画项目中用得更多，并且也更易于我们修改代码逻辑，实现定制化功能。

### 1. 使用denoising_diffusion_pytorch
**（1）训练流程**
```
pip install denoising_diffusion_pytorch
```
这个工具包中提供了 UNet 和扩散模型两个封装好的模块，你可以通过两行指令创建 UNet，并基于创建好的 UNet 创建一个完整的扩散模型，同时指定了图像分辨率和总的加噪步数。
```
from denoising_diffusion_pytorch import Unet, GaussianDiffusion
import torch 

model = Unet(
    dim = 64,  
    dim_mults = (1, 2, 4, 8)
).cuda()

diffusion = GaussianDiffusion(
    model,
    image_size = 128,
    timesteps = 1000   # number of steps
).cuda()
```
比如我们随机初始化八张图片，便可以通过后面这两行代码完成扩散模型的一次训练。
```
# 使用随机初始化的图片进行一次训练
training_images = torch.randn(8, 3, 128, 128)
loss = diffusion(training_images.cuda())
loss.backward()
```
如果想用自己本地的图像，而非随机初始化的图像，可以参考下面的代码:
```
from PIL import Image
import torchvision.transforms as transforms
import torch
# 预设一个变换操作，将PIL Image转换为PyTorch Tensor，并对其进行归一化
transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
])
# 我们认为你有个列表包含了8张图像的路径
image_paths = ['path_to_your_image1', 'path_to_your_image2', 'path_to_your_image3', 'path_to_your_image4', 
               'path_to_your_image5', 'path_to_your_image6', 'path_to_your_image7', 'path_to_your_image8'] 
# 使用List comprehension读取并处理这些图片
images = [transform(Image.open(image_path)) for image_path in image_paths] 
# 将处理好的图像List转化为一个4D Tensor，注意torch.stack能够自动处理3D Tensor到4D Tensor的转换
training_images = torch.stack(images)
# 现在training_images应该有8张3x128x128的图像
print(training_images.shape)  # torch.Size([8, 3, 128, 128])
```
训练完成后，可以直接使用得到的模型来生成图像。由于我们的模型只训练了一步，模型的输出也是纯粹的噪声图。这里只是为了让你找一下手感。
```
sampled_images = diffusion.sample(batch_size = 4)
```

**2. 数据准备**
我们以 oxford-flowers 这个数据集为例，首先需要安装 [datasets](https://huggingface.co/datasets/nelorth/oxford-flowers) 这个工具包。
```
pip install datasets
```
我们使用后面的代码就可以下载这个数据集，并将数据集中所有的图片单独存储成 png 格式，用 png 格式更方便我们查看。全部处理完大概有 8000 张图片。
```
from PIL import Image
from io import BytesIO
from datasets import load_dataset
import os
from tqdm import tqdm

dataset = load_dataset("nelorth/oxford-flowers")

# 创建一个用于保存图片的文件夹
images_dir = "./oxford-datasets/raw-images"
os.makedirs(images_dir, exist_ok=True)

# 遍历所有图片并保存，针对oxford-flowers，整个过程要持续15分钟左右
for split in dataset.keys():
    for index, item in enumerate(tqdm(dataset[split])):
        image = item['image']
        image.save(os.path.join(images_dir, f"{split}_image_{index}.png"))
```
可以在[Huggingface](https://huggingface.co/)下载自己的数据集。

**3. 模型训练**
准备工作完成，我们便可以通过以下代码来进行完整训练：
```
import torch
from denoising_diffusion_pytorch import Unet, GaussianDiffusion, Trainer

model = Unet(
    dim = 64,
    dim_mults = (1, 2, 4, 8)
).cuda()

diffusion = GaussianDiffusion(
    model,
    image_size = 128,
    timesteps = 1000   # 加噪总步数
).cuda()

trainer = Trainer(
    diffusion,
    './oxford-datasets/raw-images',
    train_batch_size = 16,
    train_lr = 2e-5,
    train_num_steps = 20000,          # 总共训练20000步
    gradient_accumulate_every = 2,    # 梯度累积步数
    ema_decay = 0.995,                # 指数滑动平均decay参数
    amp = True,                       # 使用混合精度训练加速
    calculate_fid = False,            # 我们关闭FID评测指标计算（比较耗时）。FID用于评测生成质量。
    save_and_sample_every = 2000      # 每隔2000步保存一次模型
)

trainer.train()
```
这里分享一个小技巧，如果在使用 GPU 的时候报错提示显存不足，可以通过后面的命令手工释放不再使用的 GPU 显存:
```
import gc

del old_model # 这里的old_model是指已经不会再用到的模型
gc.collect()
torch.cuda.empty_cache()
```
如果你的 GPU 不够强大，可以根据实际情况调整训练的 batch_size 大小。

对于 16G 的 V100 显卡而言，整个任务的训练要持续 3 至 4 个小时。在整个训练过程中，每次间隔 2000 个训练步，我们会保存一次模型权重，并利用当前权重进行图像的生成。随着训练步数的增多，这个扩散模型的图像生成能力在逐渐变强。

<br>

## 2. 基于diffusers训练
基于 diffusers 工具包的训练，我们要写的代码就会多得多，但可调节的参数也会多很多。

首先看数据集的使用。通过 datasets 工具包加载数据集，与 denoising_diffusion_pytorch 的训练不同，在 diffusers 训练模式下，我们不需要将数据集再转为本地图片格式。
```
import torch
from datasets import load_dataset

# 加载数据集
config.dataset_name = "huggan/smithsonian_butterflies_subset"
dataset = load_dataset(config.dataset_name, split="train")

# 封装成训练用的格式
train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)
```
为了提升模型的性能，我们可以对图像数据进行数据增广。所谓数据增广，就是对图像做一些随机左右翻转、随机颜色扰动等操作，目的是增强训练数据的多样性。
```
from torchvision import transforms

preprocess = transforms.Compose(
    [
        transforms.Resize((config.image_size, config.image_size)),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5]),
    ]
)
```
然后我们看 UNet 结构。按照下图的结构搭建 UNet 模块，比如图中输入和输出的分辨率都是 128x128，在实际 UNet 搭建中你可以任意指定。

<img src="./images/UNet train.webp" />

我们可以通过下面的代码来创建 UNet 结构:
```
from diffusers import UNet2DModel

model = UNet2DModel(
    sample_size=config.image_size,  # 目标图像的分辨率
    in_channels=3,  # 输入通道的数量，对于RGB图像为3
    out_channels=3,  # 输出通道的数量
    layers_per_block=2,  # 每个UNet块中使用的ResNet层的数量
    block_out_channels=(128, 128, 256, 256, 512, 512),  # 每个UNet块的输出通道数量
    down_block_types=( 
        "DownBlock2D",  # 常规的ResNet下采样块
        "DownBlock2D", 
        "DownBlock2D", 
        "DownBlock2D", 
        "AttnDownBlock2D",  # 具有空间自注意力的ResNet下采样块
        "DownBlock2D",
    ), 
    up_block_types=(
        "UpBlock2D",  # 常规的ResNet上采样块
        "AttnUpBlock2D",  # 具有空间自注意力的ResNet上采样块
        "UpBlock2D", 
        "UpBlock2D", 
        "UpBlock2D", 
        "UpBlock2D"  
      ),
)
```
接下来我们看采样器的用法。这里需要确定我们加噪用的采样器，帮助我们通过一步计算得到第 t 步的加噪结果。
```
from diffusers import DDPMScheduler

noise_scheduler = DDPMScheduler(num_train_timesteps=1000)

# 一步加噪的计算
noise = torch.randn(sample_image.shape)
timesteps = torch.LongTensor([50])
noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)
```
接着通过模型预测噪声，并计算损失函数。
```
import torch.nn.functional as F

noise_pred = model(noisy_image, timesteps).sample
loss = F.mse_loss(noise_pred, noise)
```
最后，我们将这些模块串联起来，便可以得到基于 diffusers 训练扩散模型的核心代码。
```
for epoch in range(num_epochs):
    for step, batch in enumerate(train_dataloader):
        clean_images = batch['images']
        # 对应于扩散模型训练过程：随机采样噪声
        noise = torch.randn(clean_images.shape).to(clean_images.device)
        bs = clean_images.shape[0]

        # 对应于扩散模型训练过程：对于batch中的每张图，随机选取时间步t
        timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bs,), device=clean_images.device).long()

        # 对应于扩散模型训练过程：一步计算加噪结果
        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)
        
        with accelerator.accumulate(model):
            # 对应于扩散模型训练过程：预测噪声值并计算损失函数
            noise_pred = model(noisy_images, timesteps, return_dict=False)[0]
            loss = F.mse_loss(noise_pred, noise)
            accelerator.backward(loss)
            optimizer.step()       
```

<br>

## 微调 Stable Diffusion
我们可以直接参考 diffusers 官方提供的[训练代码](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py)，别看这个代码有接近 1100 行，其实相比于上面提到的标准扩散模型训练，核心也只是多了 VAE 和 CLIP 的部分。

VAE 和 CLIP 部分的代码:
```
tokenizer = CLIPTokenizer.from_pretrained(
    args.pretrained_model_name_or_path, subfolder="tokenizer", revision=args.revision
)

text_encoder = CLIPTextModel.from_pretrained(
    args.pretrained_model_name_or_path, subfolder="text_encoder", revision=args.revision
)

vae = AutoencoderKL.from_pretrained(
    args.pretrained_model_name_or_path, subfolder="vae", revision=args.revision
)

unet = UNet2DConditionModel.from_pretrained(
    args.pretrained_model_name_or_path, subfolder="unet", revision=args.non_ema_revision
)

# 将vae 和 text_encoder的参数冻结，训练过程中权重不更新
vae.requires_grad_(False)
text_encoder.requires_grad_(False)
```
代码中发现了一个 tokenizer 变量。它的作用，是对我们输入的 prompt 进行分词后获取 token_id。有了 token_id，我们便可以获取模型可用的词嵌入向量。CLIP 模型的文本编码器（text_encoder）基于词嵌入向量，便可以提取文本特征。VAE 模块和 CLIP 模块都不需要权重更新，因此上面的代码中将梯度（grad）设置为 False。

<img src="./images/tokenizer embbeding.webp" />


最后，我们再看看 Stable Diffusion 训练的核心代码:
```
 for epoch in range(num_train_epochs):
     for step, batch in enumerate(train_dataloader):
     
         # VAE模块将图像编码到潜在空间
         latents = vae.encode(batch["pixel_values"].to(weight_dtype)).latent_dist.sample()
         
         # 随机噪声 & 加噪到第t步
         noise = torch.randn_like(latents)
         timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps)
         noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
         
         # 使用CLIP将文本描述作为输入
         encoder_hidden_states = text_encoder(batch["input_ids"])[0]
         target = noise
         
         # 预测噪声并计算loss
         model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample
         loss = F.mse_loss(model_pred.float(), target.float(), reduction="mean")
         optimizer.step()
```
如果你想进一步确认文本描述如何通过交叉注意力机制起作用，我推荐你去看看 [UNet2DConditionModel](https://github.com/huggingface/diffusers/blob/v0.19.3/src/diffusers/models/unet_2d_condition.py#L66) 这个模块的代码，加深理解。


<br>

## 如何调用各种 SD 模型？
```
import torch
from diffusers import DiffusionPipeline
from diffusers import DDIMScheduler, DPMSolverMultistepScheduler, EulerAncestralDiscreteScheduler
pipeline = DiffusionPipeline.from_pretrained("gsdf/Counterfeit-V2.5")
```
第四行的模型 ID 可以灵活调整，你可以切换成你心仪的模型。

然后用下面代码完成切换采样器，prompt 设置等操作，便可以随心所欲地创作了。
```
# 切换为DPM采样器
pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)

prompt = "((masterpiece,best quality)),1girl, solo, animal ears, rabbit"
negative_prompt = "EasyNegative, extra fingers,fewer fingers,"
images = pipeline(prompt, width = 512, height = 512, num_inference_steps=20, guidance_scale=7.5).images
```