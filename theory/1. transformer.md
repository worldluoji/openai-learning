# transformer

Transformer 模型的架构就是一个 seq2seq 架构，由多个 Encoder Decoder 堆叠而成

<img src="./images/Transfomer%20High-Level%20Look.png" />

概括来说，我们输入法语：je suis étudiant，经过六个 Encoder 后得到了类似于 Context Vector 的东西，
然后将得到的向量放进 Decoder 中，每个 Decoder 会对上一个 Decoder 的输出进行 Self-Attention 处理，
同时会把得到的结果与 Encoder 传递过来的 Vector 进行 Encoder-Decoder Attention 处理，将结果放入前馈网络中，这算是一个 Decoder，
而把六个 Decoder 叠加起来学习，便可得到最后的结果。这里叠起来的编解码器的数量不是固定的。

每个 Encoder Decoder 长什么样子可以看下图，原本编解码的基本单元是 RNN ，这里改用了 Self-attention layer 和 Feed Forward, 而 Decoder 则由 Self-Attention、Encoder-Decoder Attention、 Feed Forward 组成。 
Transformer 其实就是 seq2seq model with self-attention。

<img src="./images/Transformer%20Encoder-Decoder.png" />

<br>

## Encoder

<img src="./images/Encoder-Decoder%20workflow.png" />

我们输入了两个编码后的向量 x1和 x2，其中 x1是对单词 Thinking 的表示，x2是对 Machines 单词的表示。 
通过 Encoder 模块得到了两个向量 r1和 r2, 虽然也分别代表 Thinking、Machines 单词的信息，但是r1和 r2是加权后的结果，
也就是说, r1 中不仅仅包含 Thinking 单词的信息，而且还有 Machines 单词的信息，只不过 Thinking 单词信息占的比重可能很高，毕竟单词和单词本身的相关性是很高的（这里为了方便理解，距离一个例子，具体权重如何分配的是模型学习出来的）。
这里用两个词语举例子，如果输入的句子单词很多，可能不同单词之间的相关度就不一样，最后得到的向量分配的权重也就不同。

<img src="./images/words%20relation.png" />

如山图，颜色越深表示单词间相关度越高。那么怎样才能将每个单词的信息按不同权重整合起来呢 ？Self-Attention 机制。

<br>

## Self-Attention
Self-Attention的细节如下图所示，当单词 Thinking、Machines 进行 Embedding 后，分别与矩阵 
WQ、WK、WV相乘。例如 Thinking 单词 Embedding 后变成 X1向量，此向量与 WQ相乘后为q1向量，也称为 Queries。
X1与 WK 相乘得到 k1 向量，以此类推。

我们称q1, k1, v1向量分别为 Queries、Keys、Values 向量。

<img src="./images/self-attention.png" />

其中，softmax计算如下：

<img src="./images/self-attention%20softmax.png" />

经过一系列运算，最后z1 = v1 + v2, 于是 z1 便包含了两个单词的信息，只不过 Thinking 单词的信息占的比重更大一些，而 Machines 单词的信息占的比例较小。


矩阵WQ、WK、WV是学习出来的，我们试图去学习三个矩阵 WQ、WK、WV，与 Embedding 向量相乘后得到 Query，Key，Value 向量。
而期望得到的 Query，Key，Value 向量最契合当前的任务。

因为矩阵 WQ、WK、WV是学习出来的，所以得到的 Query，Key，Value 向量是比较抽象的。在这里，可以简单理解为矩阵的功能相当于抽取特征。
这里的命名 Query，Key，Value 也非常有意思，大家自己想想每个向量的功能就能对应上了。

<br>

## Multi-headed attention
刚刚的例子，我们有三个矩阵 WQ、WK、WV 与单词的 embedding 相乘，如果不仅仅是这三个矩阵呢 ？ 
比如 WQ1、WK1、WV1，WQ2、WK2、WV2等等，这样就不仅仅得到一个Z, 还会有 Z1,Z2等等。

<img src="./images/multi-head%20attention.png" />

multi-headed attention 能够使得信息更加丰富。
比如，如果不是多头的注意力机制，it 和 the animal 是相关度最高的，这符合我们的预期。
但根据句子中 it was too tired 可知，it 除了指代 the animal 还是 tired 的。
如果再引入一个 attention layer，这个 layer 就可能捕获 it 与 tired 的相关度。

multi-headed attention 全流程如下：
<img src="./images/multi-head%20attention%20workflow.png" />


<br>

## Why Transformer Important
- Attention is all you need:
The first transduction model relying entirely on self-attention to compute representations of its
input and output without using sequence-aligned RNNs or convolution

- GPU-friendly parallel computation: These layers are highly parallelizable, resulting in lower computational costs

- Sentence-level representations: Combined with positional encoding, the Transformer can capture long-range dependencies by
considering the relative positions of tokens.

<br>

## reference
- https://zhuanlan.zhihu.com/p/264468193