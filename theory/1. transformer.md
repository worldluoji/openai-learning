# transformer
Transformer 是当下公认的处理时序任务的最强模型，最初提出是为了解决自然语言处理领域的问题，比如机器翻译等任务。最近几年，Transformer 广泛应用在语音领域、计算机视觉领域等，表现不俗。特别是当下的 AI 绘画模型和 GPT 模型，也都依赖了 Transformer 模块。

## 背景
在深度学习中，有很多需要处理时序数据的任务，比如语音识别、文本理解、机器翻译、音乐生成等。
RNN（循环神经网络）、LSTM（长短时记忆网络）以及 Transformers 这些解决时序任务的方案便应运而生。

RNN 和 LSTM 解决序列问题RNN 专为处理序列数据而设计，可以灵活地处理不同长度的数据。RNN 的主要特点是在处理序列数据时，对前面的信息会产生某种“记忆”，通过这种记忆效果，RNN 可以捕捉序列中的时间依赖关系。这种“记忆”在 RNN 中被称为隐藏状态（hidden state）。

传统的 RNN 存在一个关键的问题，即“长时依赖问题”——当序列很长时，RNN 在处理过程中会出现梯度消失（梯度趋近于 0）或梯度爆炸（梯度趋近于无穷大）现象。这种情况下，RNN 可能无法有效地捕捉长距离的时间依赖信息。

为了解决这个问题， LSTM 这种特殊的 RNN 结构就派上用场了。LSTM 通过加入遗忘门、记忆门和输出门来处理长时依赖问题。这些门有助于 LSTM 更有效地保留和更新序列中的长距离信息。但是也有它的局限，主要是三个方面：
- 第一，并行计算问题。由于 LSTM 需要递归地处理序列数据，所以在计算过程中无法充分利用并行计算资源，处理长序列数据时效率较低。
- 第二，长时依赖问题。虽然 LSTM 有效地改善了传统 RNN 中的长时依赖问题，但在处理特别长的序列时，仍然可能出现依赖关系捕捉不足的问题。
- 第三，复杂性高。LSTM 相比简单的 RNN 结构更复杂，增加了网络参数和计算量，这在一定程度上影响了训练速度和模型性能。

## transformer架构
在 2017 年由 Google 提出的 Transformer，是一种基于自注意力机制（self-attention）的模型，它有效解决了 RNN 类方法的并行计算和长时依赖两大痛点。

Transformer 模型的架构就是一个 seq2seq 架构，由多个 Encoder 、Decoder 堆叠而成，通过这两个部分完成对输入序列的表示学习和输出序列的生成。

总之，编码器负责对输入序列进行抽象表示，解码器根据这些表示构建合适的输出序列。

<img src="./images/Transfomer%20High-Level%20Look.png" />

概括来说，我们输入法语：je suis étudiant，经过六个 Encoder 后得到了类似于 Context Vector 的东西，
然后将得到的向量放进 Decoder 中，每个 Decoder 会对上一个 Decoder 的输出进行 Self-Attention 处理，
同时会把得到的结果与 Encoder 传递过来的 Vector 进行 Encoder-Decoder Attention 处理，将结果放入前馈网络中，这算是一个 Decoder，而把六个 Decoder 叠加起来学习，便可得到最后的结果。这里叠起来的编解码器的数量不是固定的。

每个 Encoder Decoder 长什么样子可以看下图，原本编解码的基本单元是 RNN ，这里改用了 Self-attention layer 和 Feed Forward, 而 Decoder 则由 Self-Attention、Encoder-Decoder Attention、 Feed Forward 组成。 
Transformer 其实就是 seq2seq model with self-attention。

<img src="./images/Transformer%20Encoder-Decoder.png" />

<br>

## Encoder

<img src="./images/Encoder-Decoder%20workflow.png" />

我们输入了两个编码后的向量 x1和 x2，其中 x1是对单词 Thinking 的表示，x2是对 Machines 单词的表示。 
通过 Encoder 模块得到了两个向量 r1和 r2, 虽然也分别代表 Thinking、Machines 单词的信息，但是r1和 r2是加权后的结果，
也就是说, r1 中不仅仅包含 Thinking 单词的信息，而且还有 Machines 单词的信息，只不过 Thinking 单词信息占的比重可能很高，毕竟单词和单词本身的相关性是很高的（这里为了方便理解，距离一个例子，具体权重如何分配的是模型学习出来的）。
这里用两个词语举例子，如果输入的句子单词很多，可能不同单词之间的相关度就不一样，最后得到的向量分配的权重也就不同。

<img src="./images/words%20relation.png" />

如山图，颜色越深表示单词间相关度越高。那么怎样才能将每个单词的信息按不同权重整合起来呢 ？Self-Attention 机制。

<br>

## Self-Attention
Self-Attention的细节如下图所示，当单词 Thinking、Machines 进行 Embedding 后，分别与矩阵 
WQ、WK、WV相乘。例如 Thinking 单词 Embedding 后变成 X1向量，此向量与 WQ相乘后为q1向量，也称为 Queries。
X1与 WK 相乘得到 k1 向量，以此类推。

我们称q1, k1, v1向量分别为 Queries、Keys、Values 向量。

<img src="./images/self-attention.png" />

其中，softmax计算如下：

<img src="./images/self-attention%20softmax.png" />

经过一系列运算，最后z1 = v1 + v2, 于是 z1 便包含了两个单词的信息，只不过 Thinking 单词的信息占的比重更大一些，而 Machines 单词的信息占的比例较小。


矩阵WQ、WK、WV是学习出来的，我们试图去学习三个矩阵 WQ、WK、WV，与 Embedding 向量相乘后得到 Query，Key，Value 向量。
而期望得到的 Query，Key，Value 向量最契合当前的任务。

因为矩阵 WQ、WK、WV是学习出来的，所以得到的 Query，Key，Value 向量是比较抽象的。在这里，可以简单理解为矩阵的功能相当于抽取特征。
这里的命名 Query，Key，Value 也非常有意思，大家自己想想每个向量的功能就能对应上了。

<br>

## Multi-headed attention
刚刚的例子，我们有三个矩阵 WQ、WK、WV 与单词的 embedding 相乘，如果不仅仅是这三个矩阵呢 ？ 
比如 WQ1、WK1、WV1，WQ2、WK2、WV2等等，这样就不仅仅得到一个Z, 还会有 Z1,Z2等等。

<img src="./images/multi-head%20attention.png" />

multi-headed attention 能够使得信息更加丰富。
比如，如果不是多头的注意力机制，it 和 the animal 是相关度最高的，这符合我们的预期。
但根据句子中 it was too tired 可知，it 除了指代 the animal 还是 tired 的。
如果再引入一个 attention layer，这个 layer 就可能捕获 it 与 tired 的相关度。

multi-headed attention 全流程如下：
<img src="./images/multi-head%20attention%20workflow.png" />


<br>

## Decoder
Decoder 中的模块和 Encoder 中的模块类似，都是 Attention 层、前馈网络层、融合归一化层，不同的是 Decoder 中多了一个 Encoder-Decoder Attention 层。

<img src="./images/Encoder-Decoder%20workflow.png" />

这里先明确一下 Decoder 模块的输入输出和解码过程：
- 输出：对应i位置的输出词的概率分布
- 输入：Encoder 模块的输出 & 对应 i - 1 位置 Decoder 模块的输出。所以中间的 Encoder-Decoder Attention 不是 self-attention，它的 K，V 来自 Encoder 模块，Q 来自上一位置 Decoder 模块的输出
- 解码：这里要特别注意一下，编码可以并行计算，一次性全部encoding出来，但解码不是一次把所有序列解出来的，而是像rnn一样一个一个解出来的，因为要用上一个位置的输入当作 attention 的 query

输入序列经过 encoder 部分，然后将最上面的 encoder 的输出变换成一组 attention 向量 K 和 V, 这些向量会用于每个 decoder 的 encoder-decoder attention 层，有助于解码器聚焦在输入序列中的合适位置。

<br>

## Why Transformer Important
- Attention is all you need:
The first transduction model relying entirely on self-attention to compute representations of its
input and output without using sequence-aligned RNNs or convolution

- GPU-friendly parallel computation: These layers are highly parallelizable, resulting in lower computational costs

- Sentence-level representations: Combined with positional encoding, the Transformer can capture long-range dependencies by
considering the relative positions of tokens.

<br>

## reference
- https://zhuanlan.zhihu.com/p/264468193
- https://zhuanlan.zhihu.com/p/338817680