# transformer
Transformer 是当下公认的处理时序任务的最强模型，最初提出是为了解决自然语言处理领域的问题，比如机器翻译等任务。最近几年，Transformer 广泛应用在语音领域、计算机视觉领域等，表现不俗。特别是当下的 AI 绘画模型和 GPT 模型，也都依赖了 Transformer 模块。

## 背景
在深度学习中，有很多需要处理时序数据的任务，比如语音识别、文本理解、机器翻译、音乐生成等。
RNN（循环神经网络）、LSTM（长短时记忆网络）以及 Transformers 这些解决时序任务的方案便应运而生。

RNN 和 LSTM 解决序列问题RNN 专为处理序列数据而设计，可以灵活地处理不同长度的数据。RNN 的主要特点是在处理序列数据时，对前面的信息会产生某种“记忆”，通过这种记忆效果，RNN 可以捕捉序列中的时间依赖关系。这种“记忆”在 RNN 中被称为隐藏状态（hidden state）。

传统的 RNN 存在一个关键的问题，即“长时依赖问题”——当序列很长时，RNN 在处理过程中会出现梯度消失（梯度趋近于 0）或梯度爆炸（梯度趋近于无穷大）现象。这种情况下，RNN 可能无法有效地捕捉长距离的时间依赖信息。

为了解决这个问题， LSTM 这种特殊的 RNN 结构就派上用场了。LSTM 通过加入遗忘门、记忆门和输出门来处理长时依赖问题。这些门有助于 LSTM 更有效地保留和更新序列中的长距离信息。但是也有它的局限，主要是三个方面：
- 第一，并行计算问题。由于 LSTM 需要递归地处理序列数据，所以在计算过程中无法充分利用并行计算资源，处理长序列数据时效率较低。
- 第二，长时依赖问题。虽然 LSTM 有效地改善了传统 RNN 中的长时依赖问题，但在处理特别长的序列时，仍然可能出现依赖关系捕捉不足的问题。
- 第三，复杂性高。LSTM 相比简单的 RNN 结构更复杂，增加了网络参数和计算量，这在一定程度上影响了训练速度和模型性能。

## transformer架构
在 2017 年由 Google 提出的 Transformer，是一种基于自注意力机制（self-attention）的模型，它有效解决了 RNN 类方法的并行计算和长时依赖两大痛点。

Transformer 模型的架构就是一个 seq2seq 架构，由多个 Encoder 、Decoder 堆叠而成，通过这两个部分完成对输入序列的表示学习和输出序列的生成。

总之，编码器负责对输入序列进行抽象表示，解码器根据这些表示构建合适的输出序列。

<img src="./images/Transfomer%20High-Level%20Look.png" />

概括来说，我们输入法语：je suis étudiant，经过六个 Encoder 后得到了类似于 Context Vector 的东西，
然后将得到的向量放进 Decoder 中，每个 Decoder 会对上一个 Decoder 的输出进行 Self-Attention 处理，
同时会把得到的结果与 Encoder 传递过来的 Vector 进行 Encoder-Decoder Attention 处理，将结果放入前馈网络中，这算是一个 Decoder，而把六个 Decoder 叠加起来学习，便可得到最后的结果。这里叠起来的编解码器的数量不是固定的。

每个 Encoder Decoder 长什么样子可以看下图，原本编解码的基本单元是 RNN ，这里改用了 Self-attention layer 和 Feed Forward, 而 Decoder 则由 Self-Attention、Encoder-Decoder Attention、 Feed Forward 组成。 
Transformer 其实就是 seq2seq model with self-attention。

<img src="./images/Transformer%20Encoder-Decoder.png" />

<br>

## Encoder

<img src="./images/Encoder-Decoder%20workflow.png" />

我们输入了两个编码后的向量 x1和 x2，其中 x1是对单词 Thinking 的表示，x2是对 Machines 单词的表示。 
通过 Encoder 模块得到了两个向量 r1和 r2, 虽然也分别代表 Thinking、Machines 单词的信息，但是r1和 r2是加权后的结果，
也就是说, r1 中不仅仅包含 Thinking 单词的信息，而且还有 Machines 单词的信息，只不过 Thinking 单词信息占的比重可能很高，毕竟单词和单词本身的相关性是很高的（这里为了方便理解，距离一个例子，具体权重如何分配的是模型学习出来的）。
这里用两个词语举例子，如果输入的句子单词很多，可能不同单词之间的相关度就不一样，最后得到的向量分配的权重也就不同。

<img src="./images/words%20relation.png" />

如山图，颜色越深表示单词间相关度越高。那么怎样才能将每个单词的信息按不同权重整合起来呢 ？
[Attention 机制](./0.%20attention.md)。

<br>

## Decoder
Decoder 中的模块和 Encoder 中的模块类似，都是 Attention 层、前馈网络层、融合归一化层，不同的是 Decoder 中多了一个 Encoder-Decoder Attention 层。

<img src="./images/Encoder-Decoder%20workflow.png" />

这里先明确一下 Decoder 模块的输入输出和解码过程：
- 输出：对应i位置的输出词的概率分布
- 输入：Encoder 模块的输出 & 对应 i - 1 位置 Decoder 模块的输出。所以中间的 Encoder-Decoder Attention 不是 self-attention，它的 K，V 来自 Encoder 模块，Q 来自上一位置 Decoder 模块的输出
- 解码：这里要特别注意一下，编码可以并行计算，一次性全部encoding出来，但解码不是一次把所有序列解出来的，而是像rnn一样一个一个解出来的，因为要用上一个位置的输入当作 attention 的 query

输入序列经过 encoder 部分，然后将最上面的 encoder 的输出变换成一组 attention 向量 K 和 V, 这些向量会用于每个 decoder 的 encoder-decoder attention 层，有助于解码器聚焦在输入序列中的合适位置。

<br>

## Why Transformer Important
- Attention is all you need:
The first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution

- GPU-friendly parallel computation: These layers are highly parallelizable, resulting in lower computational costs

- Sentence-level representations: Combined with positional encoding, the Transformer can capture long-range dependencies by
considering the relative positions of tokens.

Transformer 不依赖于递归的序列运算，而是使用自注意力机制同时处理整个序列，这使得 Transformer 在处理长序列数据时速度更快，更易于并行计算。

自注意力机制允许 Transformer 直接关注序列中任意距离的依赖关系，不再受制于之前的隐状态传递。这样，Transformer 可以更好地捕捉长距离依赖关系。

然而，需要注意的是，尽管 Transformer 在很多任务中表现出优越性能，但它的训练通常需要大量的数据，对内存和计算资源的需求通常较高。另外，LSTM 和 Transformer 在特定任务上可能具有各自的优势，我们仍然需要根据具体问题和数据情况来选择最合适的模型。

<br>

## reference
- https://zhuanlan.zhihu.com/p/264468193
- https://zhuanlan.zhihu.com/p/338817680