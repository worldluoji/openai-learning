# attention
注意力机制，实际就是将有限的注意力集中在重点信息上，从而节省资源，快速获得最有效的信息。

图书管（source）里有很多书（value），为了方便查找，我们给书做了编号（key）。当我们想要了解漫威（query）的时候，我们就可以看看那些动漫、电影、甚至二战（美国队长）相关的书籍。
为了提高效率，并不是所有的书都会仔细看，针对漫威来说，动漫，电影相关的会看的仔细一些（权重高），但是二战的就只需要简单扫一下即可（权重低）。
当我们全部看完后就对漫威有一个全面的了解了。

<img src="./images/attention.png" />

Attention 原理的3步分解：
- 第一步：query 和 key 进行相似度计算，得到权值
- 第二步：将权值进行归一化，得到直接可用的权重
- 第三步：将权重和 value 进行加权求和
  
```
从上面的建模，我们可以大致感受到 Attention 的思路简单，四个字“带权求和”就可以高度概括，大道至简。做个不太恰当的类比，人类学习一门新语言基本经历四个阶段：死记硬背（通过阅读背诵学习语法练习语感）->提纲挈领（简单对话靠听懂句子中的关键词汇准确理解核心意思）->融会贯通（复杂对话懂得上下文指代、语言背后的联系，具备了举一反三的学习能力）->登峰造极（沉浸地大量练习）。
这也如同attention的发展脉络，RNN 时代是死记硬背的时期，attention 的模型学会了提纲挈领，进化到 transformer，融汇贯通，具备优秀的表达学习能力，再到 GPT、BERT，通过多任务大规模学习积累实战经验，战斗力爆棚。
要回答为什么 attention 这么优秀？是因为它让模型开窍了，懂得了提纲挈领，学会了融会贯通。
——阿里技术
```

<br>

## Attention 的 N 种类型
Attention 有很多种不同的类型.

根据是否划分层次关系，分为单层attention，多层attention和多头attention：
- 1）单层Attention，这是比较普遍的做法，用一个query对一段原文进行一次attention。
- 2）多层Attention，一般用于文本具有层次关系的模型，假设我们把一个document划分成多个句子，在第一层，我们分别对每个句子使用attention计算出一个句向量（也就是单层attention）；在第二层，我们对所有句向量再做attention计算出一个文档向量（也是一个单层attention），最后再用这个文档向量去做任务。
- 3）多头Attention（multi-head attention），这是Attention is All You Need中提到的multi-head attention，用到了多个query对一段原文进行了多次attention，每个query都关注到原文的不同部分(相关性可能不止一种)，相当于从不同的相关性进行多次单层attention, 得到多个attention score。

<img src="./images/multi-head%20attention.png" />

<br>

## 注意力机制的特点和优势
1. 注意力机制有助于克服循环神经网络（RNNs）的一些挑战，例如输
入序列长度增加时性能下降和顺序处理输入导致的计算效率低下。

2. 在自然语言处理（NLP）、计算机视觉（Computer Vision）、跨模
态任务和推荐系统等多个领域中，注意力机制已成为多项任务中的最
先进模型，取得了显著的性能提升。

3. 注意力机制不仅可以提高主要任务的性能，还具有其他优势。它们被
广泛用于提高神经网络的可解释性，帮助解释模型的决策过程，使得
原本被认为是黑盒模型的神经网络变得更易解释。这对于人们对机器
学习模型的公平性、可追溯性和透明度的关注具有重要意义。

<br>

## reference
- https://zhuanlan.zhihu.com/p/91839581