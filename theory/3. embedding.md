# Embedding
在自然语言处理中，embedding是一种技术，它将高维度的数据（如文字、图片、音频）映射到低维度空间的过程。
这种映射使得我们可以将输入的数据表示成一个连续的数值空间中的点。
简单来说，embedding就是一个N维的实值向量，它可以将任何类型的数据映射到这个数值空间中，从而让我们可以用这个向量来表示这个数据的特征。

在大模型领域，embedding被广泛应用。例如，在自然语言处理中，我们可以将每个单词或短语映射为一个实值的向量，
这个向量就包含了该单词或短语的语义信息。这样，我们就可以利用这些向量来计算单词或短语之间的相似度，或者用它们作为输入来训练模型，从而让模型可以理解和处理自然语言任务。

embedding的好处主要有以下几点：
- 降低输入数据的维度：通过将高维度的数据映射到低维度空间，embedding可以减少计算量和内存消耗，使得模型可以更高效地处理输入数据。
- 提取输入数据的语义信息：通过将单词或短语映射到向量空间中，embedding可以利用语料库中的上下文信息来学习这些单词或短语的语义信息和特征，从而增强模型的表达能力和泛化能力。
- 解决长文本输入问题：通过将长文本分成多个片段，并用embedding编码上下文信息，可以使模型更好地处理长文本任务，例如阅读理解和问答等。


## 表示学习 vs 嵌入
表示学习（Representation Learning）和嵌入（Embedding）是密切相关的概念，它们可以被视为在不同领域
中对同一概念的不同命名或描述。
- 表示学习：通过学习算法自动地从原始数据中学习到一种表示形式或特征表示，该表示形式能够更好地表达
数据的重要特征和结构。表示学习的目标是将输入数据转换为具有良好表示能力的特征空间，使得在该空间
中的数据具有更好的可分性、可解释性或推理能力。
- 嵌入：表示学习的一种形式，通常用于将高维数据映射到低维空间中的表示形式。嵌入可以是词嵌入、图像
嵌入、图嵌入等。例如，在自然语言处理中，词嵌入将词语映射到低维向量空间，以捕捉词语之间的语义和
句法关系。在图像处理中，图像嵌入将图像映射到低维向量空间，以表示图像的视觉特征。
因此，嵌入可以被视为一种表示学习的特定形式，旨在将高维数据转换为低维向量表示。表示学习可以涉及更
广泛的概念和方法，包括嵌入在内，以实现对数据的更好理解和表达

<br>

## Word Embedding
下面是一个word embedding的例子：

<img src="./images/word%20embedding.png" />

我们可以通过将两个无法比较的文字映射成向量，接下来就能实现对他们的计算。

例如：
```
queen（皇后）= king（国王）- man（男人）+ woman（女人）
```
这样计算机能明白，“皇后啊，就是女性的国王呗！”
```
walked（过去式）= walking（进行时）- swimming（进行时）+ swam（过去式）
```
同理计算机也能明白，“walked，就是walking的过去式啦！”
另外，向量间的距离也可能会建立联系，比方说“北京”是“中国”的首都，“巴黎”是“法国”的首都，那么向量：|中国|-|北京|=|法国|-|巴黎|

<br>

## Word Embedding vs Language Model
- Word Embedding：词嵌入通常被用来生成词的向量表示，这个过程通常是静态的，即一旦训练完成，每个词的向量表
示就确定了。词嵌入的主要目标是捕获单词或短语的语义和语法信息，并将这些信息以向量形式表示出来。词嵌入的
一个重要特性是，语义上相近的词在嵌入空间中的距离也比较近。然而，词嵌入并不能理解上下文信息，即相同的词
在不同的上下文中可能有不同的含义，但词嵌入无法区分这些含义。
- Language Model：语言模型则是预测词序列的概率模型，这个过程通常是动态的，会根据输入的上下文进行变化。语
言模型的主要目标是理解和生成文本。这包括对上下文的理解，词的预测，句子的生成等等。语言模型会用到词嵌
入，但同时也会对上下文进行建模，这样可以处理词在不同上下文中的不同含义。
在某种程度上，你可以将词嵌入看作是语言模型的一部分或者输入，语言模型使用词嵌入捕捉的信息，来进行更深层次的
语义理解和文本生成。当然，现在有一些更先进的模型，比如BERT，GPT等，它们生成的是上下文相关的词嵌入，即词
的嵌入会根据上下文变化，这样一定程度上弥补了传统词嵌入模型的不足。

<br>

## reference
- https://zhuanlan.zhihu.com/p/164502624