# 构建基于TQA模式的AI Agent
TQA（Thought-Question-Answer）, 其主要用途是收集人的反馈，用以提高生成内容的质量。
TQA 是从推理行动（Reason-Act，ReACT）演化出来的交互模式。

<br>

## 推理行动（Reason-Act）
ReAct 是一种专门<strong>为了 LLM 能执行某些操作任务而设计的模式</strong>。
旨在让大语言模型不仅仅局限于理解和生成文本，而是能够根据上下文进行推理（Reason），即理解语境中的复杂关系和隐含信息，
并在此基础上采取行动（Act），也就是执行任务、作出决策或与环境进行交互。

举个例子，当 LLM 收到一个问题时，它可以选择执行一个动作来检索相关信息，然后利用检索到的信息来回答问题。

ChatGPT Plugin，还有 Microsoft 开源的 Semantic Kernel 都是基于 ReAct 模式。

一个典型的 ReAct Prompting 通常包含四个部分：
- 上下文提示：提示 LLM 当前解决问题的上下文是什么。这几乎在所有 LLM 提示模版中都会用到。目的是让 LLM 开始了解我们到底想让它做什么。
- ReAct 步骤（迭代反馈与纠正）：推理和行动规划的步骤。在所有 ReAct 提示中，思考 - 行动 - 观察（Thought-Action-Observation）是一个标准顺序。ReAct 方法可能会涉及循环式的提问与回答过程，允许模型根据先前的答案进行迭代改进或细化，类似于人类在逐步解决复杂问题时的思考过程。反馈机制可能包括对模型答案的评估和调整，促使模型在下一轮迭代中更加接近正确或期望的答案。
- 推理：通过“根据现状进行思考和推理”这样的通用指令或“让我们一步步思考”这样的思维链（Chain-of-Thought, CoT）提示来实现推理。这部分通常还会包含少样本示例（few shots example），展示它如何一步步得出结论，这有助于提高模型的透明度和可解释性。
- 行动：最后一个关键信息是行动指令集，LLM 可以从中选择一个行动来响应推理思考。


例如LangChain 的 ReAct 模板：
```
Answer the following questions as best you can. You have access to the following tools:
{tools}
 
Use the following format:
 
Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
… (this Thought/Action/Action Input/Observation can repeat N times)
//…(思考 / 行动 / 行动输入 / 观察 可以循环多次）
Thought: I now know the final answer
Final Answer: the final answer to the original input question
 
Begin!
 
Question: {input}
Thought:{agent_scratchpad}
```
使用类似下面的代码，就可以真正运行这个 agent（当然，需要提供真实的 Tool，这部分请自行查阅 LangChain 或其他框架的文档）
```
import openai
import os
from langchain.llms import OpenAI
from langchain.agents import load_tools
from langchain.agents import initialize_agent
from dotenv import load_dotenv
load_dotenv()

os.environ["OPENAI_API_KEY"] = getpass()
from langchain import OpenAI, Wikipedia
from langchain.agents import initialize_agent, Tool
from langchain.agents import AgentType

tools = [
    Tool(
        name="Search",
        func=...,
        description="useful for when you need to ask with search"
    ),
    Tool(
        name="Calculate",
        func=...,
        description="useful for when you need to calculate"
    )
]
llm = OpenAI(temperature=0, model_name="gpt-3.5-turbo")
react = initialize_agent(tools, llm, agent=AgentType.REACT_DOCSTORE, verbose=True)
react.run("淘宝去年双十一的营业额，可以买多少瓶可乐")
```

<br>

## 想 - 问 - 答（Thought-Question-Answer）
理解了推理行动模式，那么想 - 问 - 答只不过提供了<strong>不同的响应步骤</strong>而已。那么用于验收条件编写的 TQA 提示词是这样的：
```
You are a business analyst who is familiar with specification by example.  I’m the domain expert.
// 你是一个业务分析师，而我是领域专家
 
===CONTEXT
{context}
===END OF CONTEXT
 
===USER STORY
{story}
===END OF USER STORY
 
Explain the user story as scenarios. Use the following format:
// 使用 场景 解释用户故事，并遵循如下格式
 
Thought: you should always think about what is still uncertain about the user story. Ignore technical concerns.
// 思考：你应该考虑用户故事中不清晰的部分。但忽略技术细节
Question: the question to ask to clarify the user story
// 问题：提出问题帮助你澄清这个用户故事
Answer: the answer I responded to the question
// 回答：我给出答案
… (this Thought/Question/Answer repeat at least 3 times, at most 10 times)
//（Thought/Question/Answer 重复至少 3 次而不多于 10 次）
Thought: I know enough to explain the user story
// 思考：我已经对这个用户故事了解了足够多的内容
Scenarios: List all possible scenarios with concrete example in
Given/When/Then style
// 场景：列出所有场景。使用 Given/When/Then 的格式表述
{history}
{input}
```
这是一个典型的 ReAct 形式的提示词，四部分分别对应的是：
- 上下文提示：你是一个业务分析师，而我是领域专家。与一般的 ReAct 不同，我们的上下文提示中，还包含了业务背景和具体的用户故事
- ReAct 步骤：Thought/Question/Answer
- 推理：就是展开解释 Thought，Question，Answer 分别要做什么
- 行动：行动只有一个，就是 Answer（向人来问问题）


因为是连续发问，我们不希望 LLM 反复询问同一个问题，也需要保持记录之前提问的内容，所以包含了历史记录（{history}）


另一方面，想要实现人与 LLM 的互动问答，需要使用 LLM API 中的停止序列（Stop Sequence）。
在模型生成内容的过程中，如果碰到指定的停止序列，模型就会停在那个位置。

在 TQA 中，停止序列是 “Answer:”，也就是碰到需要等待用户给予反馈的时候停下，等用户输入完成之后，再继续后续的推理和任务。