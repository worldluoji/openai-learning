# LLM如何辅助软件交付

## 通过自动化带来的效率提升
通过 LLM 辅助不同的认知模式，一个显而易见的效率提升是由于知识外化带来的复用。·
```
需求背景
=======
{requirements}

API要求
=====
API返回的结果是json格式；
当查找的SKU不存在时，返回404；
按关键搜索功能使用POST而不是GET；

任务
===
按照API要求为需求设计RESTful API接口。使用RAML描述。
```
在这个模板中 API 要求的部分与需求背景部分是正交的。
也就是说，无论针对何种需求背景，都可以依照同样的 API 要求，进行任务的操作。
那么，对于 API 要求这部分的知识提取，就是可复用的知识。


改变任务的部分，就能获得新的任务模板:
```
需求背景
=======
{requirements}

API要求
=====
API返回的结果是json格式；
当查找的SKU不存在时，返回404；
按关键搜索功能使用POST而不是GET；

任务
===
按照API要求为当前需求设计API客户端。
```
常规的自动化脚本是自动化一个任务，而有了 LLM 的帮助，我们通过提供相应的知识，即可完成对于一类任务的自动化。

<br>

## 通过知识传递带来效率的提高
在庞杂模式下，自动化程度并不高。庞杂模式下首先要通过知识生成对齐思路。然后再根据思路（任务列表），指导 LLM 进行后续的任务。
那么我们难以直接看到最终结果，因而感觉效率不高。

在团队中，对于同一个问题，有的人处在庞杂认知模式，而另外一些人则处在复杂的认知模式中。
效率的根源在于知识传递的效率，即知识传递的准确性，一致性和及时性，这些极大地影响着团队的效率。

过 LLM 以思维链形式提取的不可言说知识，可以以复用的形式实现知识的高效传递:
```
需求背景
=======
{requirements}

API
===
{API}

要求
===
1. 使用JAX-RS框架实现API接口
2. 对于数据库的访问通过repository接口
3. 针对每一个领域对象，构造representation model表示其返回的json数据
4. 在API接口中，通过repository访问数据库，并将数据转化为对应的representation model
5. 划分任务时，按照repository，representation model和API的顺序梳理任务

任务
===
现在我们要按照要求，根据需求实现API中提及的RESTful API。
请先生成任务列表。
```

仔细查看思维链(要求)中的内容，你就会发现，这其实是对于架构的简要描述。
同样与所处理的需求与 API 设计无关。而是从架构的角度出发，描述在当前架构模式下，拆分任务的方法。

只要我们在团队中，共享同样的思维链，那么团队对于架构就形成了一致的认知；而当架构修改时，只要更新所使用的模板，架构的改变就能准确且及时地传递。

对于个人而言，庞杂模式下的 LLM 交互模式并不能带来太多的效率提升，但考虑到团队中认知分布的不均衡，通过 LLM 依然可以对整个团队带来效率的巨大提升。

<br>

## 通过缩短反馈周期带来效率的提高
处在复杂认知模式时，我们通过探测（Probe）- 感知（Sense）- 响应（Respond）构成一个反馈循环学习新的知识，并在学习中处理要解决的任务。
其中，探测可能是最花费时间的环节，但真正发挥作用的是感知和响应环节。

由于探测环节费力费时，往往会成为整个反馈周期的瓶颈。而 LLM 在复杂认知模式下，主要负责在探测阶段帮助我们执行任务产生初始结果，或是根据反馈重新执行任务。
在 LLM 的帮助下，我们可以更快进入感知与响应的环节。在响应环节中，当我们提出反馈之后，LLM 也可以快速给出新的答案。
因而 LLM 消除了反馈循环中的瓶颈，也就极大提高了我们在复杂认知模式下的行为效率。

比如，我们可以直接执行 ChatGPT 返回的结果，验证是否符合我们的预期。
如果执行有错，可以将错误直接贴回，让 ChatGPT 继续改正。这就极大地缩短了反馈周期。

通常我们认为复杂认知模式是一种低效的认知模式。因而在传统的管理方法中，我们会尽量地控制复杂认知模式的使用，转而追求以流程为核心，处于清晰认知模式的官僚机制，或是以专家为核心、处于庞杂认知模式的技术官僚机制。
然而在 LLM 的帮助下，对于某些场景而言，复杂认知模式的效率变得可以接受了。也就是只使用复杂认知模式而无需关注认知提升，依然可以高效地解决问题。